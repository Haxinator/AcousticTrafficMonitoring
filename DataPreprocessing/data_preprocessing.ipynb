{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e53897e-fe40-43bc-936e-1848e4326c3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' import os\\nos.chdir(\"D:/data\")  # 切换到 D 盘的 data 目录\\nprint(os.getcwd()) '"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" import os\n",
    "os.chdir(\"D:/data\")  # 切换到 D 盘的 data 目录\n",
    "print(os.getcwd()) \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35b69fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "BASE_DIR = os.path.dirname(os.path.abspath('__file__'))\n",
    "UNIFIED_CSV = os.path.join(BASE_DIR, \"all_audio_labels.csv\")\n",
    "VEHICLE_SEG_DIR = os.path.join(BASE_DIR,\"vehicle_segments\")\n",
    "VEHICLE_SEG_CSV = os.path.join(VEHICLE_SEG_DIR,\"vehicle_clips.csv\")\n",
    "VEHICLE_SEG_BG_DIR = os.path.join(VEHICLE_SEG_DIR, \"background\")\n",
    "VEHICLE_SEG_BG_CLIPS_CSV = os.path.join(VEHICLE_SEG_BG_DIR, \"vehicle_clips_background.csv\")\n",
    "VEHICLE_SEG_CV_AUG_DIR = os.path.join(BASE_DIR, \"cv_aug\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "935b7390-ee5e-4cbd-8df8-c1529195e302",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved unified label file: /home/timon-l/devel/CITS5551/AcousticTrafficMonitoring/DataPreprocessing/all_audio_labels.csv\n"
     ]
    }
   ],
   "source": [
    "loc_folders = [f\"loc{i}\" for i in range(1, 7)]\n",
    "splits = [\"train\", \"val\"]\n",
    "\n",
    "all_entries = []\n",
    "\n",
    "for loc in loc_folders:\n",
    "    for split in splits:\n",
    "        csv_path = os.path.join(BASE_DIR, loc, f\"{split}.csv\")\n",
    "        if not os.path.exists(csv_path):\n",
    "            print(f\"Missing: {csv_path}\")\n",
    "            continue\n",
    "\n",
    "        df = pd.read_csv(csv_path)\n",
    "\n",
    "        # Calculate total number of cars and CVs\n",
    "        df[\"car_total\"] = df[\"car_left\"] + df[\"car_right\"]\n",
    "        df[\"cv_total\"] = df[\"cv_left\"] + df[\"cv_right\"]\n",
    "\n",
    "        # Assign label based on vehicle type\n",
    "        def classify(row):\n",
    "            if row[\"car_total\"] > 0 and row[\"cv_total\"] == 0:\n",
    "                return 0  # only car\n",
    "            elif row[\"cv_total\"] > 0 and row[\"car_total\"] == 0:\n",
    "                return 1  # only CV\n",
    "            else:\n",
    "                return 2  # mixed or none\n",
    "\n",
    "        df[\"label\"] = df.apply(classify, axis=1)\n",
    "\n",
    "        # Generate absolute path (path field includes .flac already)\n",
    "        df[\"abs_path\"] = df[\"path\"].apply(lambda x: os.path.join(BASE_DIR, loc, x.replace(\"/\", os.sep)))\n",
    "\n",
    "        # Add metadata\n",
    "        df[\"loc\"] = loc\n",
    "        df[\"split\"] = split\n",
    "\n",
    "        all_entries.append(df[[\"abs_path\", \"label\", \"loc\", \"split\"]])\n",
    "\n",
    "# Combine all rows\n",
    "all_data = pd.concat(all_entries, ignore_index=True)\n",
    "\n",
    "# Save to unified CSV\n",
    "all_data.to_csv(UNIFIED_CSV, index=False)\n",
    "print(f\"Saved unified label file: {UNIFIED_CSV}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a0f9367a-bef1-4dd0-8907-b5a21e2febf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10076/10076 [07:24<00:00, 22.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Saved segmented vehicle clips to /home/timon-l/devel/CITS5551/AcousticTrafficMonitoring/DataPreprocessing/vehicle_segments/vehicle_clips.csv\n"
     ]
    }
   ],
   "source": [
    "import librosa\n",
    "import soundfile as sf\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "def normalize_energy(energy_array):\n",
    "    return (energy_array - np.min(energy_array)) / (np.max(energy_array) - np.min(energy_array) + 1e-8)\n",
    "\n",
    "# Process a .flac file and extract 1s segments above energy threshold\n",
    "def process_flac_and_save_segments(row, output_base, segment_duration=1.0, sr=16000, baseline_secs=2, threshold_ratio=0.2):\n",
    "    abs_path = row[\"abs_path\"]\n",
    "    label = int(row[\"label\"])\n",
    "    loc = row[\"loc\"]\n",
    "    split = row[\"split\"]\n",
    "\n",
    "    if label not in [0, 1]:\n",
    "        return []\n",
    "\n",
    "    try:\n",
    "        y, _ = librosa.load(abs_path, sr=sr)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {abs_path}: {e}\")\n",
    "        return []\n",
    "\n",
    "    samples_per_seg = int(sr * segment_duration)\n",
    "    num_segments = len(y) // samples_per_seg\n",
    "    energies = [np.sqrt(np.mean(y[i*samples_per_seg:(i+1)*samples_per_seg]**2)) for i in range(num_segments)]\n",
    "    energies = np.array(energies)\n",
    "    energies_norm = normalize_energy(energies)\n",
    "    baseline = np.mean(energies_norm[:baseline_secs])\n",
    "    threshold = baseline + threshold_ratio\n",
    "    smoothed = np.convolve(energies_norm, np.ones(3)/3, mode='same')\n",
    "    active = smoothed > threshold\n",
    "\n",
    "    entries = []\n",
    "    for i, is_active in enumerate(active):\n",
    "        if not is_active:\n",
    "            continue\n",
    "\n",
    "        start = i * samples_per_seg\n",
    "        end = start + samples_per_seg\n",
    "        segment = y[start:end]\n",
    "\n",
    "        class_dir = \"car\" if label == 0 else \"cv\"\n",
    "        filename = f\"{loc}_{split}_{os.path.basename(abs_path).replace('.flac','')}_sec{i+1:02d}.wav\"\n",
    "        out_dir = os.path.join(output_base, class_dir)\n",
    "        os.makedirs(out_dir, exist_ok=True)\n",
    "        out_path = os.path.join(out_dir, filename)\n",
    "\n",
    "        sf.write(out_path, segment, sr)\n",
    "        entries.append({\"filepath\": out_path, \"label\": label})\n",
    "\n",
    "    return entries\n",
    "\n",
    "# ==== Main Process ====\n",
    "df = pd.read_csv(UNIFIED_CSV)\n",
    "df = df[df[\"label\"].isin([0, 1])]  # process only pure vehicle samples\n",
    "\n",
    "all_entries = []\n",
    "\n",
    "for _, row in tqdm(df.iterrows(), total=len(df)):\n",
    "    entries = process_flac_and_save_segments(row, VEHICLE_SEG_DIR)\n",
    "    all_entries.extend(entries)\n",
    "\n",
    "# Save final CSV\n",
    "df_out = pd.DataFrame(all_entries)\n",
    "df_out.to_csv(VEHICLE_SEG_CSV, index=False)\n",
    "print(f\"\\n✅ Saved segmented vehicle clips to {VEHICLE_SEG_CSV}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "81be3c9d-6484-4d25-94b4-3e207915f7a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of car segments (label=0): 108827\n",
      "Number of cv segments (label=1):  1778\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(VEHICLE_SEG_CSV)\n",
    "\n",
    "car_count = (df[\"label\"] == 0).sum()\n",
    "cv_count = (df[\"label\"] == 1).sum()\n",
    "\n",
    "print(f\"Number of car segments (label=0): {car_count}\")\n",
    "print(f\"Number of cv segments (label=1):  {cv_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f7e150a-c351-4f6f-8833-1d151f207164",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 392/15013 [00:32<20:15, 12.03it/s]"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Path config\n",
    "\"\"\" segments_csv = \"D:/DATA/vehicle_segments/vehicle_clips.csv\"\n",
    "original_meta_csv = \"D:/DATA/all_audio_labels.csv\"\n",
    "output_dir = \"D:/DATA/vehicle_segments/background\"\n",
    "output_csv = \"D:/DATA/vehicle_segments/vehicle_clips_background.csv\" \"\"\"\n",
    "segment_duration = 1.0\n",
    "sr = 16000\n",
    "segment_samples = int(sr * segment_duration)\n",
    "\n",
    "# Load CSVs\n",
    "df_segments = pd.read_csv(VEHICLE_SEG_CSV)\n",
    "df_meta = pd.read_csv(UNIFIED_CSV)\n",
    "\n",
    "# Index for used vehicle seconds\n",
    "segment_index = {}\n",
    "for path in df_segments[\"filepath\"]:\n",
    "    fname = os.path.basename(path)\n",
    "    parts = fname.split(\"_sec\")\n",
    "    if len(parts) != 2:\n",
    "        continue\n",
    "    key = \"_\".join(parts[0].split(\"_\")[:3])  # loc1_train_00001\n",
    "    sec = int(parts[1].replace(\".wav\", \"\"))\n",
    "    segment_index.setdefault(key, set()).add(sec)\n",
    "\n",
    "# Process each original .flac and extract unused segments\n",
    "all_background_entries = []\n",
    "os.makedirs(VEHICLE_SEG_BG_DIR, exist_ok=True)\n",
    "\n",
    "for _, row in tqdm(df_meta.iterrows(), total=len(df_meta)):\n",
    "    abs_path = row[\"abs_path\"]\n",
    "    loc = row[\"loc\"]\n",
    "    split = row[\"split\"]\n",
    "    base = os.path.basename(abs_path).replace(\".flac\", \"\")\n",
    "    key = f\"{loc}_{split}_{base}\"\n",
    "\n",
    "    try:\n",
    "        y, _ = librosa.load(abs_path, sr=sr)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {abs_path}: {e}\")\n",
    "        continue\n",
    "\n",
    "    total_segments = len(y) // segment_samples\n",
    "    active_secs = segment_index.get(key, set())\n",
    "\n",
    "    for sec in range(1, total_segments + 1):\n",
    "        if sec in active_secs:\n",
    "            continue  # Skip used vehicle seconds\n",
    "\n",
    "        start = (sec - 1) * segment_samples\n",
    "        end = start + segment_samples\n",
    "        segment = y[start:end]\n",
    "\n",
    "        outname = f\"{key}_sec{sec:02d}.wav\"\n",
    "        outpath = os.path.join(VEHICLE_SEG_BG_DIR, outname)\n",
    "        sf.write(outpath, segment, sr)\n",
    "\n",
    "        all_background_entries.append({\n",
    "            \"filepath\": outpath.replace(\"\\\\\", \"/\"),\n",
    "            \"label\": 2  # background\n",
    "        })\n",
    "\n",
    "# Save background label file\n",
    "df_out = pd.DataFrame(all_background_entries)\n",
    "df_out.to_csv(VEHICLE_SEG_BG_CLIPS_CSV, index=False)\n",
    "print(f\"\\n✅ Saved background clips: {len(df_out)} to {VEHICLE_SEG_BG_CLIPS_CSV}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56c89d0-8423-4b0c-a7d9-820b9811e6ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 增强完成！生成 CV 增强样本：98222 条，已写入 vehicle_clips.csv\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Config\n",
    "target_total = 100000\n",
    "sr = 16000\n",
    "segment_duration = 1.0\n",
    "samples_per_segment = int(sr * segment_duration)\n",
    "\n",
    "# Paths\n",
    "\"\"\" base_dir = os.path.dirname(os.path.abspath('__file__'))\n",
    "original_csv = os.path.join(base_dir, \"vehicle_clips.csv\")\n",
    "background_dir = os.path.join(base_dir, \"background\")\n",
    "cv_aug_dir = os.path.join(base_dir, \"cv_aug\") \"\"\"\n",
    "os.makedirs(VEHICLE_SEG_CV_AUG_DIR, exist_ok=True)\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv(VEHICLE_SEG_CSV)\n",
    "df_cv = df[df[\"label\"] == 1].reset_index(drop=True)\n",
    "df_car = df[df[\"label\"] == 0]\n",
    "\n",
    "# Background pool\n",
    "background_files = [os.path.join(VEHICLE_SEG_BG_DIR, f) for f in os.listdir(VEHICLE_SEG_BG_DIR) if f.endswith(\".wav\")]\n",
    "\n",
    "# Augmentation functions\n",
    "def augment_with_background(y, bg, snr_db=5):\n",
    "    if len(bg) < len(y):\n",
    "        bg = np.tile(bg, int(np.ceil(len(y)/len(bg))))\n",
    "    bg = bg[:len(y)]\n",
    "    rms_y = np.sqrt(np.mean(y**2))\n",
    "    rms_bg = np.sqrt(np.mean(bg**2))\n",
    "    alpha = rms_y / (10**(snr_db/20)) / (rms_bg + 1e-6)\n",
    "    return y + alpha * bg\n",
    "\n",
    "def time_stretch(y, rate):\n",
    "    return librosa.effects.time_stretch(y, rate)\n",
    "\n",
    "def pitch_shift(y, sr, n_steps):\n",
    "    return librosa.effects.pitch_shift(y, sr=sr, n_steps=n_steps)\n",
    "\n",
    "def reverse(y):\n",
    "    return y[::-1]\n",
    "\n",
    "# Start augmentation loop\n",
    "aug_entries = []\n",
    "existing = len(df_cv)\n",
    "needed = target_total - existing\n",
    "cv_paths = df_cv[\"filepath\"].tolist()\n",
    "\n",
    "while len(aug_entries) < needed:\n",
    "    for path in cv_paths:\n",
    "        if len(aug_entries) >= needed:\n",
    "            break\n",
    "        try:\n",
    "            y, _ = librosa.load(path, sr=sr)\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "        aug_y = y.copy()\n",
    "\n",
    "        # Random augmentation combos\n",
    "        if random.random() < 0.6 and background_files:\n",
    "            bg_path = random.choice(background_files)\n",
    "            bg_y, _ = librosa.load(bg_path, sr=sr)\n",
    "            aug_y = augment_with_background(aug_y, bg_y)\n",
    "\n",
    "        if random.random() < 0.5:\n",
    "            rate = random.uniform(0.9, 1.1)\n",
    "            try:\n",
    "                aug_y = time_stretch(aug_y, rate)\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "        if random.random() < 0.5:\n",
    "            steps = random.randint(-2, 2)\n",
    "            aug_y = pitch_shift(aug_y, sr=sr, n_steps=steps)\n",
    "\n",
    "        if random.random() < 0.3:\n",
    "            aug_y = reverse(aug_y)\n",
    "\n",
    "        # Save augmented segment\n",
    "        base_name = os.path.splitext(os.path.basename(path))[0]\n",
    "        aug_name = f\"{base_name}_aug{len(aug_entries)}.wav\"\n",
    "        aug_path = os.path.join(VEHICLE_SEG_CV_AUG_DIR, aug_name)\n",
    "        sf.write(aug_path, aug_y, sr)\n",
    "\n",
    "        aug_entries.append({\n",
    "            \"filepath\": aug_path.replace(\"\\\\\", \"/\"),\n",
    "            \"label\": 1\n",
    "        })\n",
    "\n",
    "# Save final CSV\n",
    "df_aug = pd.DataFrame(aug_entries)\n",
    "df_final = pd.concat([df, df_aug], ignore_index=True)\n",
    "df_final.to_csv(VEHICLE_SEG_CSV, index=False)\n",
    "print(f\"✅ Augmentation completed! Generated {len(df_aug)} CV samples and saved to vehicle_clips.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5716a621-b637-43bc-b853-f5e3a8299488",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
