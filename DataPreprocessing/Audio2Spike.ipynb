{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6416755d-0fa5-444c-bdc9-e6a1ac70c1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rockpool.devices.xylo.syns61201 import AFESim\n",
    "from rockpool.timeseries import TSContinuous\n",
    "import librosa\n",
    "import numpy as np\n",
    "import os\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "import soundfile as sf\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06d0d62a-1334-45c7-a24c-e6736c2429d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bitstruct\n",
      "  Using cached bitstruct-8.20.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.4 kB)\n",
      "Using cached bitstruct-8.20.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (83 kB)\n",
      "Installing collected packages: bitstruct\n",
      "Successfully installed bitstruct-8.20.0\n"
     ]
    }
   ],
   "source": [
    "!pip install bitstruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af49f2f-e2c2-4512-956c-3006d5780908",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total to process: 2927 segments\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2927/2927 [50:15<00:00,  1.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done: 2927/2927 succeeded across all tasks.\n"
     ]
    }
   ],
   "source": [
    "from rockpool.devices.xylo.syns61201 import AFESim\n",
    "from rockpool.timeseries import TSContinuous\n",
    "import librosa\n",
    "import numpy as np\n",
    "import os\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "import soundfile as sf\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from glob import glob\n",
    "from multiprocessing import Pool, cpu_count\n",
    "import random\n",
    "\n",
    "# === AFESim audio to spike encoding ===\n",
    "def audio_to_features(input_path: str, output_dir: str, label: int, target_sr: int = 16000, plot: bool = False):\n",
    "    fs = 110e3\n",
    "    raster_period = 10e-3\n",
    "    max_spike_per_raster_period = 15\n",
    "\n",
    "    if not os.path.exists(input_path):\n",
    "        logging.error(f\"File does not exist: {input_path}\")\n",
    "        return '', None\n",
    "\n",
    "    y, sr = librosa.load(input_path, sr=target_sr, mono=True)\n",
    "    dt = 1.0 / target_sr\n",
    "    ts = TSContinuous.from_clocked(y, dt=dt, name='Audio input')\n",
    "\n",
    "    afe = AFESim(\n",
    "        fs=fs,\n",
    "        raster_period=raster_period,\n",
    "        max_spike_per_raster_period=max_spike_per_raster_period,\n",
    "        add_noise=True,\n",
    "        add_offset=True,\n",
    "        add_mismatch=True,\n",
    "        seed=None\n",
    "    ).timed()\n",
    "\n",
    "    features, _, _ = afe(ts, record=True)\n",
    "    raster = features.raster(dt=raster_period, add_events=True)\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    base = os.path.splitext(os.path.basename(input_path))[0]\n",
    "    out_name = f\"{base}.npy\"\n",
    "    out_path = os.path.join(output_dir, out_name)\n",
    "\n",
    "    return out_path, raster\n",
    "\n",
    "def validate_npy_file(npy_path: str, max_channels: int = 16) -> bool:\n",
    "    try:\n",
    "        data = np.load(npy_path)\n",
    "        if not isinstance(data, np.ndarray) or data.ndim != 2:\n",
    "            return False\n",
    "        if data.shape[1] > max_channels or not np.issubdtype(data.dtype, np.integer):\n",
    "            return False\n",
    "        if np.isnan(data).any():\n",
    "            return False\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def process_clip(args):\n",
    "    input_path, split, data_dir = args\n",
    "    out_path = None\n",
    "    try:\n",
    "        cls = Path(input_path).parent.name.lower()\n",
    "        label_map = {'car': 0, 'cv': 1, 'background': 2, 'cv_aug': 1}\n",
    "        class_map = {0: 'Car', 1: 'CommercialVehicle', 2: 'Background'}\n",
    "\n",
    "        if cls not in label_map:\n",
    "            raise ValueError(f\"Unknown class '{cls}'\")\n",
    "        label = label_map[cls]\n",
    "        class_name = class_map[label]\n",
    "\n",
    "        output_dir = os.path.join(data_dir, split, class_name)\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "        out_path, raster = audio_to_features(input_path, output_dir, label)\n",
    "        if not out_path:\n",
    "            raise RuntimeError(\"audio_to_features returned no path\")\n",
    "\n",
    "        np.save(out_path, raster)\n",
    "        if not validate_npy_file(out_path):\n",
    "            os.remove(out_path)\n",
    "            return False\n",
    "        return True\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"[FAIL] {input_path}: {e}\")\n",
    "        if out_path and os.path.exists(out_path):\n",
    "            os.remove(out_path)\n",
    "        return False\n",
    "\n",
    "def stratify_paths(paths, frac=0.15, seed=42):\n",
    "    random.seed(seed)\n",
    "    random.shuffle(paths)\n",
    "    n = len(paths)\n",
    "    n_val = int(frac * n)\n",
    "    n_test = n_val\n",
    "    n_train = n - n_val - n_test\n",
    "    return paths[:n_train], paths[n_train:n_train+n_val], paths[n_train+n_val:]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    sample_size = 50000\n",
    "    logging.basicConfig(\n",
    "        filename='spike_test.log',\n",
    "        level=logging.INFO,\n",
    "        format='%(asctime)s %(levelname)s %(message)s'\n",
    "    )\n",
    "    base_dir = os.path.dirname(os.path.abspath('__file__'))\n",
    "    seg_dir = os.path.join(base_dir, 'newdata')\n",
    "    input_csv = os.path.join(seg_dir, 'vehicle_clips.csv')\n",
    "    data_dir = os.path.join(base_dir, 'newdataspikes')\n",
    "    os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "    if not os.path.exists(input_csv):\n",
    "        logging.error(f\"input CSV not found at: {input_csv}\")\n",
    "        exit(1)\n",
    "\n",
    "    df = pd.read_csv(input_csv)\n",
    "    df['filepath'] = df['filepath'].apply(lambda x: os.path.join('newdata', x.replace(\"\\\\\", \"/\")))\n",
    "    df_car = df[df['label'] == 0]\n",
    "    df_cv = df[df['label'] == 1]\n",
    "    df_bg = df[df['label'] == 2]\n",
    "\n",
    "    df_car = df_car.sample(n=min(sample_size, len(df_car)), random_state=42)\n",
    "    df_cv = df_cv.sample(n=min(sample_size, len(df_cv)), random_state=42)\n",
    "    df_bg = df_bg.sample(n=min(sample_size, len(df_bg)), random_state=42)\n",
    "\n",
    "    all_paths = df_car['filepath'].tolist() + df_cv['filepath'].tolist() + df_bg['filepath'].tolist()\n",
    "\n",
    "    print(f\"Total to process: {len(all_paths)} segments\")\n",
    "\n",
    "    # ç»Ÿä¸€æ ‡è®° split ä¸º \"Full\"ï¼ˆç”¨äºŽ process_clipï¼‰\n",
    "    tasks = [(p, \"npy\", data_dir) for p in all_paths]\n",
    "\n",
    "    from tqdm import tqdm\n",
    "    from multiprocessing import Pool\n",
    "\n",
    "    # set threads\n",
    "    max_threads = 6\n",
    "\n",
    "    with Pool(processes=max_threads) as pool:\n",
    "       results = list(tqdm(pool.imap_unordered(process_clip, tasks), total=len(tasks)))\n",
    "\n",
    "    succ = sum(results)\n",
    "    tot = len(results)\n",
    "    print(f\"Done: {succ}/{tot} succeeded across all tasks.\")\n",
    "    logging.info(f\"Finished encoding â€“ success {succ}/{tot}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f08b029-2b84-43c4-94b6-b20005a60292",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” ç±»åˆ« [Car]ï¼š956 ä¸ªæ ·æœ¬\n",
      "ðŸ” ç±»åˆ« [CommercialVehicle]ï¼š971 ä¸ªæ ·æœ¬\n",
      "ðŸ” ç±»åˆ« [Background]ï¼š1000 ä¸ªæ ·æœ¬\n",
      "\n",
      "âœ… åˆå¹¶å®Œæˆï¼Œæ€»æ ·æœ¬æ•°ï¼š2927ï¼Œç»Ÿä¸€ shapeï¼š(100, 16)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# === set path ===\n",
    "base_dir = r'newdataspikes/npy'  # â† Replace it with the path where your three category directories are \n",
    "label_map = {\n",
    "    'Car': 0,\n",
    "    'CommercialVehicle': 1,\n",
    "    'Background': 2\n",
    "}\n",
    "\n",
    "target_shape = (100, 16)  # â† Set the shape of the data you wish to harmonize (e.g. from the most common samples)\n",
    "\n",
    "X_all = []\n",
    "y_all = []\n",
    "\n",
    "# === Iterate over all category files ===\n",
    "for cls_name, cls_label in label_map.items():\n",
    "    cls_folder = os.path.join(base_dir, cls_name)\n",
    "    npy_files = sorted(glob(os.path.join(cls_folder, '*.npy')))\n",
    "    print(f\"ðŸ” Class [{cls_name}]ï¼š{len(npy_files)} samples\")\n",
    "\n",
    "    for path in npy_files:\n",
    "        try:\n",
    "            data = np.load(path)\n",
    "\n",
    "            # --- è£å‰ª ---\n",
    "            if data.shape[0] > target_shape[0]:\n",
    "                data = data[:target_shape[0], :]\n",
    "\n",
    "            # --- å¡«å…… ---\n",
    "            elif data.shape[0] < target_shape[0]:\n",
    "                pad_len = target_shape[0] - data.shape[0]\n",
    "                pad = np.zeros((pad_len, data.shape[1]))\n",
    "                data = np.vstack((data, pad))\n",
    "\n",
    "            # --- ç»´åº¦å¯¹é½éªŒè¯ ---\n",
    "            if data.shape != target_shape:\n",
    "                print(f\"âš ï¸ Shape dismatchï¼Œskipï¼š{path} current shape={data.shape}\")\n",
    "                continue\n",
    "\n",
    "            X_all.append(data)\n",
    "            y_all.append(cls_label)\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ reading errorï¼š{path}ï¼Œfalseï¼š{e}\")\n",
    "\n",
    "X_all = np.stack(X_all)\n",
    "y_all = np.array(y_all)\n",
    "print(f\"\\nâœ… Merger completed, total samplesï¼š{len(X_all)}ï¼Œuniform shapeï¼š{X_all.shape[1:]}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a5f51316-cc80-4043-a9ca-8471a69e2171",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d9c8ba9c-2876-4721-8d70-4d10f500e71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_temp, X_test, y_temp, y_test = train_test_split(X_all, y_all, test_size=0.15, random_state=42, stratify=y_all)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.15, random_state=42, stratify=y_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a11d2bca-2b3d-4969-adde-7737e59bb1d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸŽ‰ æ‰€æœ‰æ–‡ä»¶ä¿å­˜å®Œæˆï¼š\n",
      " - è®­ç»ƒé›†ï¼šX_train.npy shape = (2113, 100, 16)\n",
      " - è®­ç»ƒé›†ï¼šy_train.npy shape = (2113,)\n",
      " - éªŒè¯é›†ï¼šX_val.npy shape = (374, 100, 16)\n",
      " - éªŒè¯é›†ï¼šy_val.npy shape = (374,)\n",
      " - æµ‹è¯•é›†ï¼šX_test.npy shape = (440, 100, 16)\n",
      " - æµ‹è¯•é›†ï¼šy_test.npy shape = (440,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# === save ===\n",
    "np.save(\"X_train.npy\", X_train)\n",
    "np.save(\"y_train.npy\", y_train)\n",
    "np.save(\"X_val.npy\", X_val)\n",
    "np.save(\"y_val.npy\", y_val)\n",
    "np.save(\"X_test.npy\", X_test)\n",
    "np.save(\"y_test.npy\", y_test)\n",
    "\n",
    "print(\"\\nðŸŽ‰ complete savingï¼š\")\n",
    "print(f\" - training setï¼šX_train.npy shape = {X_train.shape}\")\n",
    "print(f\" - training setï¼šy_train.npy shape = {y_train.shape}\")\n",
    "print(f\" - validation setï¼šX_val.npy shape = {X_val.shape}\")\n",
    "print(f\" - validation setï¼šy_val.npy shape = {y_val.shape}\")\n",
    "print(f\" - test setï¼šX_test.npy shape = {X_test.shape}\")\n",
    "print(f\" - test setï¼šy_test.npy shape = {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc8e33f-126c-4073-8d96-5eb46590cc25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c17fcec-8674-43ec-bbf0-687432725142",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
