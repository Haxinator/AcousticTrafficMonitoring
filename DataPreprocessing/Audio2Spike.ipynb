{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6416755d-0fa5-444c-bdc9-e6a1ac70c1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rockpool.devices.xylo.syns61201 import AFESim\n",
    "from rockpool.timeseries import TSContinuous\n",
    "import librosa\n",
    "import numpy as np\n",
    "import os\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "import soundfile as sf\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06d0d62a-1334-45c7-a24c-e6736c2429d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bitstruct\n",
      "  Using cached bitstruct-8.20.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.4 kB)\n",
      "Using cached bitstruct-8.20.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (83 kB)\n",
      "Installing collected packages: bitstruct\n",
      "Successfully installed bitstruct-8.20.0\n"
     ]
    }
   ],
   "source": [
    "!pip install bitstruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af49f2f-e2c2-4512-956c-3006d5780908",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total to process: 2927 segments\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2927/2927 [50:15<00:00,  1.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done: 2927/2927 succeeded across all tasks.\n"
     ]
    }
   ],
   "source": [
    "from rockpool.devices.xylo.syns61201 import AFESim\n",
    "from rockpool.timeseries import TSContinuous\n",
    "import librosa\n",
    "import numpy as np\n",
    "import os\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "import soundfile as sf\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from glob import glob\n",
    "from multiprocessing import Pool, cpu_count\n",
    "import random\n",
    "\n",
    "# === AFESim audio to spike encoding ===\n",
    "def audio_to_features(input_path: str, output_dir: str, label: int, target_sr: int = 16000, plot: bool = False):\n",
    "    fs = 110e3\n",
    "    raster_period = 10e-3\n",
    "    max_spike_per_raster_period = 15\n",
    "\n",
    "    if not os.path.exists(input_path):\n",
    "        logging.error(f\"File does not exist: {input_path}\")\n",
    "        return '', None\n",
    "\n",
    "    y, sr = librosa.load(input_path, sr=target_sr, mono=True)\n",
    "    dt = 1.0 / target_sr\n",
    "    ts = TSContinuous.from_clocked(y, dt=dt, name='Audio input')\n",
    "\n",
    "    afe = AFESim(\n",
    "        fs=fs,\n",
    "        raster_period=raster_period,\n",
    "        max_spike_per_raster_period=max_spike_per_raster_period,\n",
    "        add_noise=True,\n",
    "        add_offset=True,\n",
    "        add_mismatch=True,\n",
    "        seed=None\n",
    "    ).timed()\n",
    "\n",
    "    features, _, _ = afe(ts, record=True)\n",
    "    raster = features.raster(dt=raster_period, add_events=True)\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    base = os.path.splitext(os.path.basename(input_path))[0]\n",
    "    out_name = f\"{base}.npy\"\n",
    "    out_path = os.path.join(output_dir, out_name)\n",
    "\n",
    "    return out_path, raster\n",
    "\n",
    "def validate_npy_file(npy_path: str, max_channels: int = 16) -> bool:\n",
    "    try:\n",
    "        data = np.load(npy_path)\n",
    "        if not isinstance(data, np.ndarray) or data.ndim != 2:\n",
    "            return False\n",
    "        if data.shape[1] > max_channels or not np.issubdtype(data.dtype, np.integer):\n",
    "            return False\n",
    "        if np.isnan(data).any():\n",
    "            return False\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def process_clip(args):\n",
    "    input_path, split, data_dir = args\n",
    "    out_path = None\n",
    "    try:\n",
    "        cls = Path(input_path).parent.name.lower()\n",
    "        label_map = {'car': 0, 'cv': 1, 'background': 2, 'cv_aug': 1}\n",
    "        class_map = {0: 'Car', 1: 'CommercialVehicle', 2: 'Background'}\n",
    "\n",
    "        if cls not in label_map:\n",
    "            raise ValueError(f\"Unknown class '{cls}'\")\n",
    "        label = label_map[cls]\n",
    "        class_name = class_map[label]\n",
    "\n",
    "        output_dir = os.path.join(data_dir, split, class_name)\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "        out_path, raster = audio_to_features(input_path, output_dir, label)\n",
    "        if not out_path:\n",
    "            raise RuntimeError(\"audio_to_features returned no path\")\n",
    "\n",
    "        np.save(out_path, raster)\n",
    "        if not validate_npy_file(out_path):\n",
    "            os.remove(out_path)\n",
    "            return False\n",
    "        return True\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"[FAIL] {input_path}: {e}\")\n",
    "        if out_path and os.path.exists(out_path):\n",
    "            os.remove(out_path)\n",
    "        return False\n",
    "\n",
    "def stratify_paths(paths, frac=0.15, seed=42):\n",
    "    random.seed(seed)\n",
    "    random.shuffle(paths)\n",
    "    n = len(paths)\n",
    "    n_val = int(frac * n)\n",
    "    n_test = n_val\n",
    "    n_train = n - n_val - n_test\n",
    "    return paths[:n_train], paths[n_train:n_train+n_val], paths[n_train+n_val:]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    sample_size = 50000\n",
    "    logging.basicConfig(\n",
    "        filename='spike_test.log',\n",
    "        level=logging.INFO,\n",
    "        format='%(asctime)s %(levelname)s %(message)s'\n",
    "    )\n",
    "    base_dir = os.path.dirname(os.path.abspath('__file__'))\n",
    "    seg_dir = os.path.join(base_dir, 'newdata')\n",
    "    input_csv = os.path.join(seg_dir, 'vehicle_clips.csv')\n",
    "    data_dir = os.path.join(base_dir, 'newdataspikes')\n",
    "    os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "    if not os.path.exists(input_csv):\n",
    "        logging.error(f\"input CSV not found at: {input_csv}\")\n",
    "        exit(1)\n",
    "\n",
    "    df = pd.read_csv(input_csv)\n",
    "    df['filepath'] = df['filepath'].apply(lambda x: os.path.join('newdata', x.replace(\"\\\\\", \"/\")))\n",
    "    df_car = df[df['label'] == 0]\n",
    "    df_cv = df[df['label'] == 1]\n",
    "    df_bg = df[df['label'] == 2]\n",
    "\n",
    "    df_car = df_car.sample(n=min(sample_size, len(df_car)), random_state=42)\n",
    "    df_cv = df_cv.sample(n=min(sample_size, len(df_cv)), random_state=42)\n",
    "    df_bg = df_bg.sample(n=min(sample_size, len(df_bg)), random_state=42)\n",
    "\n",
    "    all_paths = df_car['filepath'].tolist() + df_cv['filepath'].tolist() + df_bg['filepath'].tolist()\n",
    "\n",
    "    print(f\"Total to process: {len(all_paths)} segments\")\n",
    "\n",
    "    # 统一标记 split 为 \"Full\"（用于 process_clip）\n",
    "    tasks = [(p, \"npy\", data_dir) for p in all_paths]\n",
    "\n",
    "    from tqdm import tqdm\n",
    "    from multiprocessing import Pool\n",
    "\n",
    "    # set threads\n",
    "    max_threads = 6\n",
    "\n",
    "    with Pool(processes=max_threads) as pool:\n",
    "       results = list(tqdm(pool.imap_unordered(process_clip, tasks), total=len(tasks)))\n",
    "\n",
    "    succ = sum(results)\n",
    "    tot = len(results)\n",
    "    print(f\"Done: {succ}/{tot} succeeded across all tasks.\")\n",
    "    logging.info(f\"Finished encoding – success {succ}/{tot}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f08b029-2b84-43c4-94b6-b20005a60292",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 类别 [Car]：956 个样本\n",
      "🔍 类别 [CommercialVehicle]：971 个样本\n",
      "🔍 类别 [Background]：1000 个样本\n",
      "\n",
      "✅ 合并完成，总样本数：2927，统一 shape：(100, 16)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# === set path ===\n",
    "base_dir = r'newdataspikes/npy'  # ← Replace it with the path where your three category directories are \n",
    "label_map = {\n",
    "    'Car': 0,\n",
    "    'CommercialVehicle': 1,\n",
    "    'Background': 2\n",
    "}\n",
    "\n",
    "target_shape = (100, 16)  # ← Set the shape of the data you wish to harmonize (e.g. from the most common samples)\n",
    "\n",
    "X_all = []\n",
    "y_all = []\n",
    "\n",
    "# === Iterate over all category files ===\n",
    "for cls_name, cls_label in label_map.items():\n",
    "    cls_folder = os.path.join(base_dir, cls_name)\n",
    "    npy_files = sorted(glob(os.path.join(cls_folder, '*.npy')))\n",
    "    print(f\"🔍 Class [{cls_name}]：{len(npy_files)} samples\")\n",
    "\n",
    "    for path in npy_files:\n",
    "        try:\n",
    "            data = np.load(path)\n",
    "\n",
    "            # --- 裁剪 ---\n",
    "            if data.shape[0] > target_shape[0]:\n",
    "                data = data[:target_shape[0], :]\n",
    "\n",
    "            # --- 填充 ---\n",
    "            elif data.shape[0] < target_shape[0]:\n",
    "                pad_len = target_shape[0] - data.shape[0]\n",
    "                pad = np.zeros((pad_len, data.shape[1]))\n",
    "                data = np.vstack((data, pad))\n",
    "\n",
    "            # --- 维度对齐验证 ---\n",
    "            if data.shape != target_shape:\n",
    "                print(f\"⚠️ Shape dismatch，skip：{path} current shape={data.shape}\")\n",
    "                continue\n",
    "\n",
    "            X_all.append(data)\n",
    "            y_all.append(cls_label)\n",
    "        except Exception as e:\n",
    "            print(f\"❌ reading error：{path}，false：{e}\")\n",
    "\n",
    "X_all = np.stack(X_all)\n",
    "y_all = np.array(y_all)\n",
    "print(f\"\\n✅ Merger completed, total samples：{len(X_all)}，uniform shape：{X_all.shape[1:]}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a5f51316-cc80-4043-a9ca-8471a69e2171",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d9c8ba9c-2876-4721-8d70-4d10f500e71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_temp, X_test, y_temp, y_test = train_test_split(X_all, y_all, test_size=0.15, random_state=42, stratify=y_all)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.15, random_state=42, stratify=y_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a11d2bca-2b3d-4969-adde-7737e59bb1d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🎉 所有文件保存完成：\n",
      " - 训练集：X_train.npy shape = (2113, 100, 16)\n",
      " - 训练集：y_train.npy shape = (2113,)\n",
      " - 验证集：X_val.npy shape = (374, 100, 16)\n",
      " - 验证集：y_val.npy shape = (374,)\n",
      " - 测试集：X_test.npy shape = (440, 100, 16)\n",
      " - 测试集：y_test.npy shape = (440,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# === save ===\n",
    "np.save(\"X_train.npy\", X_train)\n",
    "np.save(\"y_train.npy\", y_train)\n",
    "np.save(\"X_val.npy\", X_val)\n",
    "np.save(\"y_val.npy\", y_val)\n",
    "np.save(\"X_test.npy\", X_test)\n",
    "np.save(\"y_test.npy\", y_test)\n",
    "\n",
    "print(\"\\n🎉 complete saving：\")\n",
    "print(f\" - training set：X_train.npy shape = {X_train.shape}\")\n",
    "print(f\" - training set：y_train.npy shape = {y_train.shape}\")\n",
    "print(f\" - validation set：X_val.npy shape = {X_val.shape}\")\n",
    "print(f\" - validation set：y_val.npy shape = {y_val.shape}\")\n",
    "print(f\" - test set：X_test.npy shape = {X_test.shape}\")\n",
    "print(f\" - test set：y_test.npy shape = {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc8e33f-126c-4073-8d96-5eb46590cc25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c17fcec-8674-43ec-bbf0-687432725142",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
